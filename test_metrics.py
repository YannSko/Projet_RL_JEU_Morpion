"""
Script de test pour vÃ©rifier le calcul des mÃ©triques
"""

import pickle
from pathlib import Path
from rl_logic.metrics import ModelMetrics

# Charger un modÃ¨le rÃ©cent
models_dir = Path("models")
model_files = list(models_dir.glob("model_*ep_*.pkl"))

if not model_files:
    print("âŒ Aucun modÃ¨le trouvÃ©")
    exit(1)

# Prendre le plus rÃ©cent
latest_model = max(model_files, key=lambda p: p.stat().st_mtime)
print(f"ğŸ“Š Test du modÃ¨le: {latest_model.name}\n")

# Charger le modÃ¨le
with open(latest_model, 'rb') as f:
    model_data = pickle.load(f)

print("=== Contenu du modÃ¨le ===")
print(f"ClÃ©s: {model_data.keys()}\n")

print("=== MÃ©tadonnÃ©es ===")
metadata = model_data.get('metadata', {})
for key, value in metadata.items():
    print(f"  {key}: {value}")

print("\n=== Stats ===")
stats = model_data.get('stats', {})
for key, value in stats.items():
    print(f"  {key}: {value}")

print("\n=== HyperparamÃ¨tres ===")
hyperparams = model_data.get('hyperparameters', {})
for key, value in hyperparams.items():
    print(f"  {key}: {value}")

# Tester le calcul des mÃ©triques
print("\n" + "="*70)
print("CALCUL DES MÃ‰TRIQUES")
print("="*70)

try:
    # PrÃ©parer les donnÃ©es comme dans le code
    test_data = {
        'states': stats.get('total_states', 0),
        'epsilon': stats.get('epsilon', 1.0),
        'metadata': metadata
    }
    
    print(f"\nDonnÃ©es pour le calcul:")
    print(f"  states: {test_data['states']}")
    print(f"  epsilon: {test_data['epsilon']}")
    print(f"  metadata keys: {list(test_data['metadata'].keys())}")
    
    metrics = ModelMetrics.compute_all_metrics(test_data)
    
    print("\nâœ… MÃ‰TRIQUES CALCULÃ‰ES:")
    print(f"  ğŸ† Composite Score: {metrics.get('composite_score', 'N/A'):.2f}")
    print(f"  ğŸ“Š Performance Score: {metrics.get('performance_score', 'N/A'):.2f}")
    print(f"  âš¡ Efficiency Score: {metrics.get('efficiency_score', 'N/A'):.2f}")
    print(f"  ğŸ’ª Robustness Score: {metrics.get('robustness_score', 'N/A'):.2f}")
    print(f"  ğŸš€ Learning Speed: {metrics.get('learning_speed', 'N/A'):.2f}")
    
    print(f"\n  Win Rate: {metrics.get('win_rate', 'N/A'):.2f}%")
    print(f"  Draw Rate: {metrics.get('draw_rate', 'N/A'):.2f}%")
    print(f"  Loss Rate: {metrics.get('loss_rate', 'N/A'):.2f}%")
    print(f"  Ã‰tats appris: {metrics.get('states_learned', 'N/A')}")
    print(f"  Ã‰pisodes: {metrics.get('total_episodes', 'N/A')}")
    
except Exception as e:
    print(f"\nâŒ ERREUR lors du calcul:")
    print(f"  {e}")
    import traceback
    traceback.print_exc()

print("\n" + "="*70)
