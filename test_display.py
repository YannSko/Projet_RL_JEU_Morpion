"""
Test rapide pour vÃ©rifier l'affichage des mÃ©triques
"""

from rl_logic.model_manager import ModelManager
from rl_logic.metrics import ModelMetrics

print("=" * 70)
print("TEST AFFICHAGE DES MÃ‰TRIQUES")
print("=" * 70 + "\n")

# CrÃ©er le manager
manager = ModelManager()

# Lister les modÃ¨les
models = manager.list_models()

print(f"ğŸ“ {len(models)} modÃ¨les trouvÃ©s\n")

# Tester les 5 premiers modÃ¨les
for i, model in enumerate(models[:5], 1):
    print(f"\n{i}. {model['name']}")
    print(f"   Timestamp: {model.get('timestamp', 'N/A')}")
    print(f"   Ã‰tats: {model.get('states', 0)}")
    print(f"   Win Rate (racine): {model.get('final_win_rate', 'N/A')}")
    
    metadata = model.get('metadata', {})
    print(f"   Win Rate (metadata): {metadata.get('final_win_rate', 'N/A')}")
    print(f"   Total Episodes: {model.get('total_episodes', metadata.get('total_episodes', 'N/A'))}")
    
    # Tester le calcul des mÃ©triques (comme dans l'interface)
    try:
        model_data = {
            'states': model.get('states', 0),
            'epsilon': model.get('epsilon', 1.0),
            'metadata': {
                'final_win_rate': model.get('final_win_rate', metadata.get('final_win_rate', 0)),
                'final_draw_rate': model.get('final_draw_rate', metadata.get('final_draw_rate', 0)),
                'final_loss_rate': model.get('final_loss_rate', metadata.get('final_loss_rate', 0)),
                'total_episodes': model.get('total_episodes', metadata.get('total_episodes', 0)),
                **{k: v for k, v in metadata.items() if k not in ['final_win_rate', 'final_draw_rate', 'final_loss_rate', 'total_episodes']}
            },
            'timestamp': model.get('timestamp', '')
        }
        
        metrics = ModelMetrics.compute_all_metrics(model_data)
        
        if metrics and metrics.get('composite_score', 0) > 0:
            print(f"   âœ… MÃ‰TRIQUES CALCULÃ‰ES:")
            print(f"      ğŸ† Score: {metrics.get('composite_score', 0):.1f}")
            print(f"      ğŸ“Š Perf: {metrics.get('performance_score', 0):.1f}")
            print(f"      âš¡ Eff: {metrics.get('efficiency_score', 0):.1f}")
            print(f"      ğŸ’ª Rob: {metrics.get('robustness_score', 0):.2f}")
        else:
            print(f"   âŒ Pas de mÃ©triques (ancien modÃ¨le)")
            
    except Exception as e:
        print(f"   âŒ ERREUR: {e}")

print("\n" + "=" * 70)
print("RÃ©sultat: Si vous voyez âœ… au-dessus, les mÃ©triques fonctionnent !")
print("=" * 70)
