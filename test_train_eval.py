"""
Test rapide de l'entraÃ®nement avec Ã©valuation post-training
"""
from rl_logic.agent import QLearningAgent
from engine.environment import TicTacToeEnvironment
from rl_logic.trainer import Trainer
from rl_logic.model_manager import ModelManager

def test_train_with_eval():
    """Test d'un entraÃ®nement court avec Ã©valuation"""
    print("\n" + "="*70)
    print("ğŸ§ª TEST : ENTRAÃNEMENT + Ã‰VALUATION POST-TRAINING")
    print("="*70)
    
    # Initialiser
    agent = QLearningAgent(
        alpha=0.15,
        gamma=0.92,
        epsilon=1.0,
        epsilon_min=0.01,
        epsilon_decay=0.9995
    )
    env = TicTacToeEnvironment()
    manager = ModelManager()
    trainer = Trainer(agent, env, model_manager=manager)
    
    # EntraÃ®ner avec Ã©valuation automatique
    print("\nğŸ“ Lancement de l'entraÃ®nement...")
    results = trainer.train(
        num_episodes=5000,      # EntraÃ®nement court
        eval_games=200,         # 200 parties d'Ã©valuation par seed
        eval_seeds=5,           # 5 seeds diffÃ©rentes (robustesse)
        verbose=True
    )
    
    # VÃ©rifier les mÃ©tadonnÃ©es du modÃ¨le sauvegardÃ©
    print("\n" + "="*70)
    print("ğŸ“‹ VÃ‰RIFICATION DES MÃ‰TADONNÃ‰ES")
    print("="*70)
    
    models = manager.list_models()
    latest_model = models[0]  # Le plus rÃ©cent
    
    metadata = latest_model.get('metadata', {})
    
    print(f"\nğŸ“¦ ModÃ¨le: {latest_model['name']}")
    print(f"\nâœ… MÃ©triques principales (depuis Ã©valuation):")
    print(f"   â€¢ final_win_rate: {metadata.get('final_win_rate', 0):.1f}%")
    print(f"   â€¢ final_draw_rate: {metadata.get('final_draw_rate', 0):.1f}%")
    print(f"   â€¢ final_loss_rate: {metadata.get('final_loss_rate', 0):.1f}%")
    print(f"   â€¢ eval_games: {metadata.get('eval_games', 0)}")
    print(f"   â€¢ eval_seeds: {metadata.get('eval_seeds', 1)}")
    print(f"   â€¢ metrics_source: {metadata.get('metrics_source', 'N/A')}")
    
    # Statistiques de robustesse
    if 'eval_robustness' in metadata:
        robustness = metadata['eval_robustness']
        print(f"\nğŸ² Robustesse (multi-seed):")
        print(f"   â€¢ Ã‰cart-type: {robustness.get('win_rate_std', 0):.2f}%")
        print(f"   â€¢ Min: {robustness.get('win_rate_min', 0):.1f}%")
        print(f"   â€¢ Max: {robustness.get('win_rate_max', 0):.1f}%")
        
        # DÃ©tails par seed
        if robustness.get('seed_results'):
            print(f"\n   ğŸ“‹ RÃ©sultats par seed:")
            for seed_res in robustness['seed_results'][:5]:  # Afficher max 5 seeds
                print(f"      Seed {seed_res['seed']}: {seed_res['win_rate']:.1f}% "
                      f"({seed_res['wins']}/{seed_res['num_games']})")
    
    if 'training_stats' in metadata:
        train_stats = metadata['training_stats']
        print(f"\nğŸ“Š Statistiques d'entraÃ®nement (rÃ©fÃ©rence):")
        print(f"   â€¢ train_win_rate: {train_stats.get('train_win_rate', 0):.1f}%")
        print(f"   â€¢ train_draw_rate: {train_stats.get('train_draw_rate', 0):.1f}%")
        print(f"   â€¢ train_loss_rate: {train_stats.get('train_loss_rate', 0):.1f}%")
        
        # Calculer la diffÃ©rence
        eval_wr = metadata.get('final_win_rate', 0)
        train_wr = train_stats.get('train_win_rate', 0)
        diff = eval_wr - train_wr
        
        print(f"\nğŸ“ˆ DiffÃ©rence Eval - Train:")
        print(f"   Win Rate: {diff:+.1f}% ", end="")
        if diff > 5:
            print("âœ¨ (Ã©valuation bien meilleure)")
        elif diff > 0:
            print("âœ… (Ã©valuation lÃ©gÃ¨rement meilleure)")
        elif diff > -5:
            print("âš–ï¸ (similaire)")
        else:
            print("âš ï¸ (surapprentissage possible)")
    
    print("\n" + "="*70)
    print("âœ… TEST TERMINÃ‰")
    print("="*70)
    print("\nğŸ’¡ Les mÃ©triques sont maintenant basÃ©es sur l'Ã‰VALUATION (Îµ=0)")
    print("   et non sur la moyenne d'entraÃ®nement !")
    print("="*70 + "\n")

if __name__ == "__main__":
    test_train_with_eval()
