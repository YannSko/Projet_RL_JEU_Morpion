"""
Script d'analyse et de comparaison des mod√®les
Permet de comparer tous les mod√®les entra√Æn√©s et identifier le meilleur.
"""

import argparse
from pathlib import Path
from rl_logic.model_manager import ModelManager
from rl_logic.model_comparator import ModelComparator
from rl_logic.agent import QLearningAgent
from engine.environment import TicTacToeEnvironment


def analyze_all_models(models_dir: str = "models", top_n: int = 10):
    """
    Analyse compl√®te de tous les mod√®les.
    
    Args:
        models_dir: R√©pertoire des mod√®les
        top_n: Nombre de mod√®les √† afficher dans le top
    """
    print("\n" + "=" * 80)
    print(" ANALYSE COMPL√àTE DES MOD√àLES")
    print("=" * 80 + "\n")
    
    manager = ModelManager(models_dir)
    manager.analyze_models(top_n)


def export_metrics(models_dir: str = "models", output_file: str = "models/models_metrics.csv"):
    """
    Exporte toutes les m√©triques en CSV.
    
    Args:
        models_dir: R√©pertoire des mod√®les
        output_file: Fichier de sortie
    """
    print("\nüìä Export des m√©triques en cours...")
    manager = ModelManager(models_dir)
    manager.export_metrics(output_file)


def load_best_model_demo(models_dir: str = "models", metric: str = "composite_score"):
    """
    D√©montre le chargement du meilleur mod√®le.
    
    Args:
        models_dir: R√©pertoire des mod√®les
        metric: Crit√®re de s√©lection
    """
    print("\n" + "=" * 80)
    print(f" CHARGEMENT DU MEILLEUR MOD√àLE (crit√®re: {metric})")
    print("=" * 80 + "\n")
    
    # Cr√©er un agent vide
    agent = QLearningAgent()
    
    # Charger le meilleur mod√®le
    manager = ModelManager(models_dir)
    success = manager.load_best_model(agent, metric)
    
    if success:
        print("\n‚úÖ Meilleur mod√®le charg√© avec succ√®s !")
        print(f"\nStatistiques de l'agent:")
        stats = agent.get_stats()
        print(f"  √âtats dans la Q-table: {stats['total_states']}")
        print(f"  Paires √©tat-action: {stats['total_state_actions']}")
        print(f"  Q-value moyenne: {stats['avg_q_value']:.4f}")
        print(f"  Q-value max: {stats['max_q_value']:.4f}")
        print(f"  Q-value min: {stats['min_q_value']:.4f}")
        print(f"\nHyperparam√®tres:")
        print(f"  Alpha (Œ±): {agent.alpha}")
        print(f"  Gamma (Œ≥): {agent.gamma}")
        print(f"  Epsilon (Œµ): {agent.epsilon:.6f}")
    else:
        print("\n‚ùå √âchec du chargement du mod√®le")


def compare_models(models_dir: str = "models", filters: dict = None):
    """
    Compare les mod√®les avec filtres optionnels.
    
    Args:
        models_dir: R√©pertoire des mod√®les
        filters: Filtres √† appliquer
    """
    print("\n" + "=" * 80)
    print(" COMPARAISON DES MOD√àLES")
    print("=" * 80 + "\n")
    
    comparator = ModelComparator(models_dir)
    comparator.compute_metrics_for_all_models()
    
    if filters:
        print(f"Application des filtres: {filters}\n")
        filtered = comparator.filter_models(filters)
        print(f"Mod√®les correspondants: {len(filtered)}\n")
        
        for model in filtered[:20]:  # Afficher max 20
            print(f"  ‚Ä¢ {model['filename']}")
            print(f"    Score: {model['composite_score']:.2f} | "
                  f"Win: {model['win_rate']:.1f}% | "
                  f"√âtats: {model['states_learned']}")
    else:
        # Afficher le rapport complet
        comparator.generate_report()


def show_categories(models_dir: str = "models"):
    """
    Affiche les mod√®les par cat√©gories.
    
    Args:
        models_dir: R√©pertoire des mod√®les
    """
    print("\n" + "=" * 80)
    print(" CAT√âGORISATION DES MOD√àLES")
    print("=" * 80 + "\n")
    
    comparator = ModelComparator(models_dir)
    comparator.compute_metrics_for_all_models()
    categories = comparator.get_models_by_category()
    
    for category, models in categories.items():
        print(f"\n{category.upper().replace('_', ' ')} ({len(models)} mod√®les):")
        print("‚îÄ" * 80)
        
        # Afficher les 5 premiers de chaque cat√©gorie
        for model in models[:5]:
            print(f"  ‚Ä¢ {model['filename']}")
            print(f"    Win: {model['win_rate']:.1f}% | "
                  f"Score: {model['composite_score']:.1f} | "
                  f"√âpisodes: {model['total_episodes']}")


def main():
    """Point d'entr√©e principal du script."""
    parser = argparse.ArgumentParser(
        description="Analyse et comparaison des mod√®les Q-Learning pour le Morpion"
    )
    
    parser.add_argument(
        "command",
        choices=["analyze", "export", "load", "compare", "categories"],
        help="Commande √† ex√©cuter"
    )
    
    parser.add_argument(
        "--models-dir",
        default="models",
        help="R√©pertoire contenant les mod√®les (d√©faut: models)"
    )
    
    parser.add_argument(
        "--top-n",
        type=int,
        default=10,
        help="Nombre de mod√®les √† afficher dans le top (d√©faut: 10)"
    )
    
    parser.add_argument(
        "--metric",
        default="composite_score",
        choices=["composite_score", "win_rate", "performance_score", 
                "efficiency_score", "robustness_score"],
        help="Crit√®re de s√©lection du meilleur mod√®le (d√©faut: composite_score)"
    )
    
    parser.add_argument(
        "--output",
        default="models/models_metrics.csv",
        help="Fichier de sortie pour l'export (d√©faut: models/models_metrics.csv)"
    )
    
    parser.add_argument(
        "--min-win-rate",
        type=float,
        help="Filtre: win rate minimum"
    )
    
    parser.add_argument(
        "--max-episodes",
        type=int,
        help="Filtre: nombre max d'√©pisodes"
    )
    
    parser.add_argument(
        "--min-episodes",
        type=int,
        help="Filtre: nombre min d'√©pisodes"
    )
    
    args = parser.parse_args()
    
    # Construire les filtres
    filters = {}
    if args.min_win_rate:
        filters['min_win_rate'] = args.min_win_rate
    if args.max_episodes:
        filters['max_episodes'] = args.max_episodes
    if args.min_episodes:
        filters['min_episodes'] = args.min_episodes
    
    # Ex√©cuter la commande
    if args.command == "analyze":
        analyze_all_models(args.models_dir, args.top_n)
    
    elif args.command == "export":
        export_metrics(args.models_dir, args.output)
    
    elif args.command == "load":
        load_best_model_demo(args.models_dir, args.metric)
    
    elif args.command == "compare":
        compare_models(args.models_dir, filters if filters else None)
    
    elif args.command == "categories":
        show_categories(args.models_dir)


if __name__ == "__main__":
    main()
